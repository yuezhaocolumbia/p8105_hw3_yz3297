---
title: "p8105_hw3_yz3297"
author: "Yue Zhao"
date: "2018年10月12日"
output: github_document
---
#Problem 1


```{r setup}
library(tidyverse)
library(p8105.datasets)

data(brfss_smart2010)
head(brfss_smart2010)

brfssnew_data=brfss_smart2010 %>%
  janitor::clean_names()  %>%
  filter(topic=="Overall Health")   %>%
  select(year,locationabbr, locationdesc, response, data_value)%>%
  spread(key=response, value= data_value) %>%
  #clean the names, filter the overall health topic, drop other variables and turning data from long to wide
  
  janitor::clean_names()
  #clean the names again after spreading

brfssnew_data

```

Answering questions:
```{r}
filter(count(filter(brfssnew_data,year==2002),locationabbr), n==7)
#select from the freqency table in 2002 to see which states appeared at 7 locations
```
Question 1: In 2002, Connecticut, Florida and North Carolina were observed at 7 locations.

```{r}
    brfssnew_data %>%
      group_by(locationabbr,year) %>%
      summarize(n=n()) %>%
      ggplot(aes(x = year, y = n, color = locationabbr)) + 
      geom_point() + geom_line() + 
      theme(legend.position = "bottom")
```


Question 2: Above is the spaghetti plot that shows the number of observations in each state from 2002 to 2010.

```{r}
 brfssnew_data %>%
      filter(locationabbr=="NY", year== 2002 | year== 2006 | year==2010) %>%
      group_by(locationabbr,year) %>%
      summarize(mean(excellent),sd(excellent))

```

Question 3: This table shows the mean and standard deviation of the proportion of "excellent" in NY for the year 2002,2006,2010.

```{r}


    mean_data= brfssnew_data %>%
      group_by(locationabbr,year) %>%
      summarize(excellent_mean=mean(excellent),very_good_mean=mean(very_good),good_mean=mean(good),fair_mean=mean(fair),poor_mean=mean(poor)) %>%
      gather(key=response, value= data_value, excellent_mean: poor_mean) %>%
      na.omit()
      
        
    mean_data %>%
      ggplot(aes(x = year, y =data_value , color=locationabbr))  + 
      geom_line() +
      facet_grid(~response)
    

```


Question 4: This is the panel showing for each year and state, the mean proportion of each category over time. 


##Problem 2

```{r}
data(instacart)

instacart

```

The dimension of dataset instacart is `r nrow(instacart)` by `r ncol(instacart)`. The main variables are order_id, product_id, user_id, order_number, product_name , aisle_id, department. Above is the sample observations of the dataset. 

Question 1: There are `r nrow(count(instacart,aisle_id))` aisles. 


```{r}
aisle_count=count(instacart,aisle) %>%
arrange(desc(n))
print(aisle_count, n=10)
#let's sort it by descending order and print out top ten entries
```
These are the top 10 aisles the most items are ordered from.

Question 2: 

```{r}

   aisle_count2=count(instacart,aisle_id) %>%
      arrange(desc(n))
    
   aisle_count2 %>%
      ggplot(aes(x = aisle_id, y = n, color=aisle_id)) + 
      geom_bar(stat="identity", width=0.5)
```

To arrange this graph in "aisle name" seems impossible because the names are too long to show on the screen. Alternatively, we can use the aisle id instead. 

Question 3:
```{r}
    instacart %>%
     filter(aisle=="baking ingredients" | aisle=="dog food care"| aisle=="packaged vegetables fruits") %>%
     group_by(aisle) %>%
     count(product_name) %>%
     filter(n==max(n) & (aisle=="baking ingredients"|aisle=="dog food care"| aisle=="packaged vegetables fruits"))
     #choose the maximum in each category
```

This table shows the most popular item in each aisle. 


Question 4:
```{r}
     instacart %>%
       filter(product_name=="Pink Lady Apples" | product_name=="Coffee Ice Cream") %>%
       group_by(product_name,order_dow) %>%
       summarise(mean_1=mean(order_hour_of_day)) %>%
       spread(key=order_dow, value=mean_1) %>%
       rename("Sunday" = "0" ,"Monday" = "1", "Tuesday" = "2", "Wednesday" = "3", "Thursday" = "4", "Friday" = "5", "Saturday" = "6")
```

This is a 2*7 table showing for each day of the week, the mean hour of ordering for each item.


##Problem 3
```{r}
data(ny_noaa)

ny_noaa

```
The dimension of dataset instacart is `r nrow(ny_noaa)` by `r ncol(ny_noaa)`. The main variables are id,date,prcp, snow, snwd, tmax, tmin.
Above is the sample observations of the dataset. tmax and tmin are the varibles that have the most missing values. The proportion of missing data in tmax is `r 1134358/nrow(ny_noaa)*100`%. The proportion of missing data in tmin is `r 1134420/nrow(ny_noaa)*100`%. 

Question 1: 

```{r}
ny_noaa2=ny_noaa %>%
separate("date", c("Year", "Month", "Day"), sep = "-") 
```

```{r}

count(ny_noaa2, snow)

```

The most observed value for snowfall is 0 (n=2008508), 13 (n=23905) , 25 (n=31022) , NA (n=381221). Because snow do not happen in the summer, we expect to have 0 snowfall in those days. 


Question 2:

```{r}
ny_noaa3=ny_noaa2 %>%
    janitor::clean_names()  %>%
    filter(month=="01" | month=="07") %>%
    mutate(mean_temp= 0.5*(as.numeric(tmax)+as.numeric(tmin))) %>%
    group_by(id,year,month) %>%
    summarise(m_1=mean(mean_temp)) %>%
    na.omit()

#calculate the mean for each station    
  
  
ny_noaa3 %>%
ggplot(aes(x = as.numeric(year), y =m_1))  + 
    labs(
    title = "Mean Temperature plot",
    x = "Year",
    y = "Tens of Degrees (C)",
    caption = "Data from the rnoaa package"
    )  +
    geom_hex(binwidth = c(2,2)) +
    facet_grid(~month)

```

We can see in January, the average temperature is around 0 to -15 degrees Celcius. An outlier would be in 1995, the temperature goes down to -18 degrees. 

In July, the average temperature is around 15 to 25 degree Celcius. An outlier would be in 2010, the temperature goes up above 25 degrees. 




Question 3: 



```{r}
#devtools::install_github("thomasp85/patchwork")
library(patchwork)

#install.packages("hexbin")



p1<- ny_noaa2 %>%
    na.omit() %>%
    ggplot(aes(x = as.numeric(tmin), y =as.numeric(tmax))) +
    labs(
    title = "Temperature plot",
    x = "Minimum daily temperature (C)",
    y = "Maxiumum daily temperature (C)",
    caption = "Data from the rnoaa package"
    )  +
    geom_hex(binwidth = c(20, 20))


p2<- ny_noaa2 %>%
    na.omit() %>%
    filter(snow < 100 & snow > 0) %>%
    ggplot(aes(x = as.numeric(Year), y =snow, color=as.numeric(Year))) + 
    labs(
      title = "Snowfall plot",
      x = "Year",
      y = "Snowfall(mm)",
      caption = "Data from the rnoaa package"
    ) +
     geom_violin(aes(fill = Year), color = "blue", alpha = 1) +
    theme(legend.position = "none")
  
p1 + p2
```

These are the panel graphs showing tmin, tmax and snowfall. 


